# -*- coding: utf-8 -*-
"""algoritmo_eda_modelagem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/197_tFpvx2eiiRq5qORxdMlYPIIEwMiLD

# **MVGSC - [LH 2025-11] Desafio de Ciência de Dados**

---

## 0. Configuração Inicial
"""

# Instalação de bibliotecas necessárias (somente na primeira execução)
!pip install ydata-profiling scikit-learn seaborn matplotlib pandas numpy joblib

# Montagem do Google Drive para acesso aos arquívos
from google.colab import drive
drive.mount('/content/drive')

import os
print(os.getcwd()) # Exibir diretório atual
print(os.listdir()) # Listar arquivos para confirmar acessibilidade do dataset

"""## 1. Importação de bibliotecas"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from ydata_profiling import ProfileReport
from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer, StandardScaler
from sklearn.metrics import mean_absolute_error ,mean_squared_error, r2_score
from sklearn.pipeline import Pipeline
import joblib
import pickle

"""## 2. Carregamento dos dados"""

df = pd.read_csv('/content/drive/MyDrive/[LH 2025-11] Desafio de Ciência de Dados/LH_CD_MARCUS_VINICIUS_GONCALVES_DE_SOUZA_COUTO/classificacao_filmes_imdb.csv')
df.info() # Exibir informações sobre tipos de dados e valores nulos
df.describe() # Estatísticas básicas
df.head() # Primeiras linhas do dataset

"""## 3. Limpeza e tratamento dos dados"""

# Conversão da coluna "Gross" para valor numérico através da remoção de caracteres não numéricos
df['Gross'] = pd.to_numeric(df['Gross'].astype(str).str.replace('[^0-9]', '', regex=True), errors='coerce')

# Conversão da coluna "Runtime" para número através da remoção de " min"
df['Runtime'] = df['Runtime'].str.replace(' min', '', regex=False).astype(float)

# Definição de variáveis preditoras e definição de variável-alvo, respectivamente
features = ['Gross', 'Meta_score', 'Runtime', 'No_of_Votes']
target = 'IMDB_Rating'

# Remoção de linhas com valores nulos na variável-alvo
df = df.dropna(subset=[target])

# Criação de variáveis independentes (X) e dependente (y)
X = df[features].copy()
y = df[target].values

# Separação de variáveis com transformação logarítmica das demais
log_cols = ['Gross', 'No_of_Votes']
other_cols = ['Meta_score', 'Runtime']

# Pipeline para variáveis com transformação logarítmica
num_log = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('log1p', FunctionTransformer(np.log1p, validate=False)),
    ('scaler', StandardScaler())
])

# Pipeline para variáveis sem transformação logarítmica
num_other = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Combinador de pipelines em um único pré-processador de dados
preprocessor = ColumnTransformer([
    ('log_num', num_log, log_cols),
    ('other_num', num_other, other_cols)
])

"""## 4. Análise Exploratória dos  (EDA)

Foi executada a Análise Exploratória de Dados para compreender distribuições, relações e correlações entre variáveis.

**Principais descobertas:**
- `IMDB_Rating` possui média ≈ 7.94, com leve concentração entre 7 e 8.
- Os dados referentes ao número de votos e ao faturamento apresentaram assimetria considerável, portanto, foi aplicada a transformação `log1p` em ambas as variáveis.  
- Dados ausentes tratados: faturamento e média ponderada de todas as críticas tiveram valores nulos descartados.  

**Hipóteses levantadas:**
1. Popularidade (`No_of_Votes`) está diretamente associada a maior faturamento e estabilidade da nota.
2. Filmes com `Meta_score` alto possuem mais probabilidade de obter nota IMDb mais alta.
"""

# Relatório automatizado para análise exploratória de dados
profile = ProfileReport(df, title="Relatório EDA de Análise de Dados - Classificação de Filmes", minimal=True)
profile.to_file("relatorio_EDA_filmes.html")

# Exemplificação de gráfico simples: Distribuição das notas IMDb
sns.histplot(df['IMDB_Rating'], kde=True, bins=20)
plt.xlabel('Nota IMDb')
plt.ylabel('Quantidade de Filmes')
plt.title('Distribuição das notas IMDb')
plt.show()

"""### 4.1. Recomendações de Filme

Para uma pessoa que não conheço, eu recomendaria **filmes em geral com alta nota crítica**

**Critérios observados:**
- `Meta_score` ≥ 75  
- `No_of_Votes` no top 25%

Os filmes analisados obtiveram, em média, um bom faturamento histórico e notas IMDb acima de 8.0.

### 4.2. Fatores para Alto Faturamento

Principais fatores identificados para faturamento elevado:
1. **Número de votos:** quanto maior o engajamento, maior bilheteria.  
2. **Média ponderada de todas as críticas:** uma ótima recepção crítica é capaz de incrementar o faturamento do filme em questão.

Recomendação: investir em projetos com **boa crítica** e **lançamento em datas estratégicas**, além de um **elenco renomado**.

### 4.3. Insights da Coluna Overview

Durante a execução deste desafio, não foi possível desenvolver um modelo robusto para extrair informações preditivas da coluna Overview devido a limitações de tempo e recursos computacionais. Esta variável específica, em se tratando de dados textuais, exigiria uma etapa adicional de pré-processamento e a aplicação de técnicas de Processamento de Linguagem Natural (PLN), por meio de representações semânticas mais modernas, como embeddings de linguagem.

Alternativamente, realizou-se somente uma análise exploratória básica, com a utilização de regras simples com base em palavras-chave para a devida identificação de possíveis correlações entre determinados termos e gêneros específicos de filmes.

Embora tal abordagem não tenha finalidade preditiva, isto indica o potencial dessa variável para futuras análises. Em uma situação real, recomenda-se a aplicação de técnicas mais sofisticadas, combinando algoritmos de classificação e PLN, para capturar com mais eficácia as informações contidas na coluna Overview e avaliar seu impacto no desempenho do modelo.

## 5. Pré-Processamento: Treinamento e Teste
"""

# Geração de pipeline inicial com pré-processamento e modelo
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor(random_state=42))
])

# Validação cruzada com o intuito de estimar desempenho do modelo
cv = KFold(n_splits=5, shuffle=True, random_state=42)
scores = -cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)
print('CV MAE (5-fold):', scores.mean(), scores.std())

# Espaço de busca para hiperparâmetros do Random Forest
Param_dist = {
    'model__n_estimators': [100, 200, 400, 600, 800],
    'model__max_depth': [None, 10, 20, 30, 40],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 5, 10],
    'model__max_features': ['sqrt', 'log2', None]
}

# Busca aleatória com a finalidade de encontrar melhores hiperparâmetros
rnd_search = RandomizedSearchCV(
    pipeline,
    param_distributions=Param_dist,
    n_iter=10,
    cv=cv,
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    random_state=42,
    verbose=1
    )
rnd_search.fit(X, y)

# Resultados da busca
print('Melhor MAE:', -rnd_search.best_score_)
print('Melhores parâmetros:', rnd_search.best_params_)

# Pipeline final com melhores hiperparâmetros
best_pipeline = rnd_search.best_estimator_

"""## 6. Modelagem

**Tipo de problema:** Análise de Regressão (variável dependente: IMDB_Rating).  

**Modelos testados:** Regressão Linear, Árvores de Decisão e Random Forest.  

**Decisão final:** `RandomForestRegressor` → melhor opção entre robustez e desempenho.  

**Pré-processamento aplicado:**
- `log1p` nas variáveis de faturamento e número de votos para lidar com assimetria  
- Imputação de valores ausentes/nulos por mediana  

**Validação:** 5-fold Cross Validation. → Validação Cruzada
**Principal Métrica:** MAE (interpretação direta em pontos IMDb).  

**Melhores parâmetros encontrados:**  

- n_estimators=200
- min_samples_split=10
- min_samples_leaf=2
- max_features='log2'
- max_depth=None

**Resultados (5-fold CV):**  
- CV MAE: 0.1558 ± 0.0051  
- MAE final: 0.1623  
- RMSE: 0.2048  
- R²: 0.3609
"""

# Divisão em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_pipeline.fit(X_train, y_train)

"""## 7. Avaliação do Modelo"""

# Variável responsável por efetuar previsões no conjunto de teste
y_pred = best_pipeline.predict(X_test)

# Cálculos de métricas de avaliação
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred)) # Calculate RMSE by taking the square root of the mean squared error
r2 = r2_score(y_test, y_pred)

print(f'MAE: {mae:.4f}')
print(f'RMSE: {rmse:.4f}')
print(f'R²: {r2:.4f}')

# Gráficos de resíduos
resid = y_test - y_pred
plt.scatter(y_pred, resid, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Previsões')
plt.ylabel('Resíduos')
plt.title('Resíduos vs. Previsões')
plt.show()

sns.histplot(resid, kde=True, bins=20)
plt.title('Distribuição dos resíduos')
plt.show()

# Gráfico de Relevância das Variáveis
model = best_pipeline.named_steps['model']
importances = model.feature_importances_
feature_importances = pd.Series(importances, index=features).sort_values(ascending=True)
feature_importances.plot(kind='barh')
plt.title('Relevância das Variáveis')
plt.show()
for feature, importance in zip(features, importances):
    print(f'{feature}: {importance:.4f}')

"""## 8. Processamento de modelagem"""

caminho = '/content/drive/MyDrive/[LH 2025-11] Desafio de Ciência de Dados/LH_CD_MARCUS_VINICIUS_GONCALVES_DE_SOUZA_COUTO/IMDb_pipeline.pkl'

# Salvamento do arquivo referente ao pipeline completo (pré-processamento + modelo)
joblib.dump(best_pipeline, caminho)

# Descarregamento do arquivo referente ao pipeline completo (pré-processamento + modelo)
from google.colab import files
files.download(caminho)

# Teste de carregamento de arquivo
best_pipeline = joblib.load(caminho)

# Salvamento de métricas em arquivo .txt
with open('/content/drive/MyDrive/[LH 2025-11] Desafio de Ciência de Dados/LH_CD_MARCUS_VINICIUS_GONCALVES_DE_SOUZA_COUTO/metricas_modelo.txt', 'w') as f:
    f.write(f'MAE: {mae:.4f}\n'),
    f.write(f'RMSE: {rmse:.4f}\n'),
    f.write(f'R²: {r2:.4f}\n')

"""## 9. Previsão específica"""

# Função preditora de notas para um novo filme
def prever_filme(modelo, gross, meta_score, runtime, no_of_votes):
    novo_filme = pd.DataFrame({
        'Gross': [gross],
        'Meta_score': [meta_score],
        'Runtime': [runtime],
        'No_of_Votes': [no_of_votes]
    })
    return modelo.predict(novo_filme)[0]

# Exemplo de aplicação
nota_prevista = prever_filme(best_pipeline, 14835707, 85, 130, 1850982)
print(f'Nota IMDb prevista para o filme: {nota_prevista:.2f}')

"""### 9.1. Previsão para The Shawshank Redemption

Características do filme:
- Ano: 1994
- Gênero: Drama
- Duração: 142 min
- Meta_score: 80
- Votos: 2.343.110
- Faturamento: 28.341.469  

**Predição obtida:** 8.74  

**Intervalo de confiança (± MAE):** 8.58 – 8.90  

Conclusão: O modelo prevê nota **muito próxima da real (9.3)**, mostrando boa capacidade preditiva.

## 10. Referências

Referências
*   Scikit-learn documentation - https://scikit-learn.org/stable/
*   "Aurélien Géron, Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow"
*   ydata-profilling docs - https://docs.profiling.ydata.ai/latest/
*   IMDb: Um Sonho de Liberdade - https://www.imdb.com/pt/title/tt0111161/?ref_=nv_sr_srsg_0_tt_7_nm_0_in_0_q_The%2520Shawshank%2520Redemption%27
"""